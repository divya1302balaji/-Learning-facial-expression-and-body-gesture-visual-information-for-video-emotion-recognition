# -Learning-facial-expression-and-body-gesture-visual-information-for-video-emotion-recognition
 Learning facial expression and body gesture visual information for video emotion recognition
📄 Project Overview
This project focuses on recognizing human emotions from video data by analyzing both facial expressions and body gestures. It integrates advanced deep learning techniques to capture spatio-temporal features and understand emotional cues through a multi-stream architecture.

💡 Key Features
🧠 Spatio-Temporal Convolutional Model for extracting localized facial expression features from video frames.

🔁 Two-Stream LSTM Model to model global temporal relationships and fuse them with local features for accurate emotion recognition.

💃 Attention-Based Channel-Wise Convolutional Model for analyzing body gestures and their contribution to emotional context.

🔗 Fusion Mechanisms to combine facial and body cues for improved emotion classification performance.

🗂️ Repository Structure
final output (1).ipynb – Main implementation notebook with model training and evaluation.

Final-ProjectReport-Divya-Batch (1).pdf – Detailed project report.

README.md – Project documentation (this file).

🛠️ Technologies Used
Python

TensorFlow / PyTorch

OpenCV

Keras

Scikit-learn

LSTM, CNN, Attention Mechanisms

📌 Goal
To develop a robust video-based emotion recognition system by learning from both facial and bodily cues, enhancing the accuracy and interpretability of emotion recognition models in real-world scenarios.

