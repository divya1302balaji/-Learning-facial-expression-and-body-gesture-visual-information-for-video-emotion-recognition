# -Learning-facial-expression-and-body-gesture-visual-information-for-video-emotion-recognition
 Learning facial expression and body gesture visual information for video emotion recognition
ğŸ“„ Project Overview
This project focuses on recognizing human emotions from video data by analyzing both facial expressions and body gestures. It integrates advanced deep learning techniques to capture spatio-temporal features and understand emotional cues through a multi-stream architecture.

ğŸ’¡ Key Features
ğŸ§  Spatio-Temporal Convolutional Model for extracting localized facial expression features from video frames.

ğŸ” Two-Stream LSTM Model to model global temporal relationships and fuse them with local features for accurate emotion recognition.

ğŸ’ƒ Attention-Based Channel-Wise Convolutional Model for analyzing body gestures and their contribution to emotional context.

ğŸ”— Fusion Mechanisms to combine facial and body cues for improved emotion classification performance.

ğŸ—‚ï¸ Repository Structure
final output (1).ipynb â€“ Main implementation notebook with model training and evaluation.

Final-ProjectReport-Divya-Batch (1).pdf â€“ Detailed project report.

README.md â€“ Project documentation (this file).

ğŸ› ï¸ Technologies Used
Python

TensorFlow / PyTorch

OpenCV

Keras

Scikit-learn

LSTM, CNN, Attention Mechanisms

ğŸ“Œ Goal
To develop a robust video-based emotion recognition system by learning from both facial and bodily cues, enhancing the accuracy and interpretability of emotion recognition models in real-world scenarios.

